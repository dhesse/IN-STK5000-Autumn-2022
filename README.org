* IN-STK 5000 - Autumn 2022

This repository consists the course [[course][IN-STK 5000]], held at
Oslo University in autumn 2022.

* Contents

** Lectures

*** Lecture 1 - Data-based decision making

 - What is data-based decision making?
 - Why data-based decision making?
   - Industry perspective
     - What is digitalization?
       - Some examples
     - Why do we talk about digitalization now?
   - Three vs of big data
 - Methods for data-based deciion making
   - Machine learning
   - Alternatives to machine learning
 - Limitations and challenges
   - Model interpretability/explainability
   - Privacy
   - Fairness
   - Scarcity or abundance of data

** Topics

*** Introduction

*** Tools of the trade part 1 - Python, Jupyter, Pandas

The aim here is merely to get everyone on the same page. ~1-2
lectures.

- Python - Data Science's favourite programming language
- Jupyter - Notebooks
- Pandas - Industry Standard Data Analysis

*** Data and Data Quality

With the use of the tools covered in the first part, we can now look
at data and its properties and implications for data driven decision
making. The project can now already start by looking at the data and
its basic properties.

**** Basics

- Targets, what are we interested in?
- Correlation

**** Data collection

- Caveats and challenges

**** Privacy

- Sensitive variables
- GDPR
- [ ] Should we cover everything including differential privacy here,
  or do a later deep dive?

**** Outliers, Data Contamination, Fraud Detection

- Basic definitions, what is an outlier and how to identify them
  - In particular, what makes estimating tails of a distribution hard?
- Reasons for the presence or absence of outliers
- Advanced methods for outlier detection
  - Such as clustering + outlier detection, autoencoders
  - Benford's Law, fraud detection


*** Tools of the trade part 2 - Scikit-learn, Tensorflow

- Scikit-learn - Industry Standard Machine Learning Package
- Tensorflow - Accelerated Neural Networks and Bayesian Modeling


*** Beyond Machine Learning - Decision Making

- Confidence intervals, uncertainty
- Bayesian models
- Reliability and reproducibility
- Multi-armed bandits (?)

*** 'Big data' Tools: Apache Spark, Apache Airflow 

- Why do we need specific tools for large data sets?
  - What is a large data set?
- How do those tools deal with large data sets?
  - Example: Map Reduce
- Apache Spark - Big Data Machine Learning
- Apache Airflow - Model
  
*** Challenges

- Model interpretability/explainability
- Fairness
- Scarcity or abundance of data


#+LINK: course https://www.uio.no/studier/emner/matnat/ifi/IN-STK5000/index-eng.html
